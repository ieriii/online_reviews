{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_07a import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import PIL,os,mimetypes\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "image_extensions = set(k for k,v in mimetypes.types_map.items() if v.startswith('image/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def setify(o): return o if isinstance(o,set) else set(listify(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_files(p, fs, extensions=None):\n",
    "    p = Path(p)\n",
    "    res = [p/f for f in fs if not f.startswith('.')\n",
    "           and ((not extensions) or f'.{f.split(\".\")[-1].lower()}' in extensions)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_files(path, extensions=None, recurse=False, include=None):\n",
    "    path = Path(path)\n",
    "    extensions = setify(extensions)\n",
    "    extensions = {e.lower() for e in extensions}\n",
    "    if recurse:\n",
    "        res = []\n",
    "        for i,(p,d,f) in enumerate(os.walk(path)): # returns (dirpath, dirnames, filenames)\n",
    "            if include is not None and i==0: d[:] = [o for o in d if o in include]\n",
    "            else:                            d[:] = [o for o in d if not o.startswith('.')]\n",
    "            res += _get_files(p, f, extensions)\n",
    "        return res\n",
    "    else:\n",
    "        f = [o.name for o in os.scandir(path) if o.is_file()]\n",
    "        return _get_files(path, f, extensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def compose(x, funcs, *args, order_key='_order', **kwargs):\n",
    "    key = lambda o: getattr(o, order_key, 0)\n",
    "    for f in sorted(listify(funcs), key=key): x = f(x, **kwargs)\n",
    "    return x\n",
    "\n",
    "class ItemList(ListContainer):\n",
    "    def __init__(self, items, path='.', tfms=None):\n",
    "        super().__init__(items)\n",
    "        self.path,self.tfms = Path(path),tfms\n",
    "\n",
    "    def __repr__(self): return f'{super().__repr__()}\\nPath: {self.path}'\n",
    "    \n",
    "    def new(self, items, cls=None):\n",
    "        if cls is None: cls=self.__class__\n",
    "        return cls(items, self.path, tfms=self.tfms)\n",
    "    \n",
    "    def  get(self, i): return i\n",
    "    def _get(self, i): return compose(self.get(i), self.tfms)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        res = super().__getitem__(idx)\n",
    "        if isinstance(res,list): return [self._get(o) for o in res]\n",
    "        return self._get(res)\n",
    "\n",
    "class ImageList(ItemList):\n",
    "    @classmethod\n",
    "    def from_files(cls, path, extensions=None, recurse=True, include=None, **kwargs):\n",
    "        if extensions is None: extensions = image_extensions\n",
    "        return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs)\n",
    "    \n",
    "    def get(self, fn): return PIL.Image.open(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Transform(): _order=0\n",
    "\n",
    "class MakeRGB(Transform):\n",
    "    def __call__(self, item): return item.convert('RGB')\n",
    "\n",
    "def make_rgb(item): return item.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def grandparent_splitter(fn, valid_name='valid', train_name='train'):\n",
    "    gp = fn.parent.parent.name\n",
    "    return True if gp==valid_name else False if gp==train_name else None\n",
    "\n",
    "def split_by_func(items, f):\n",
    "    mask = [f(o) for o in items]\n",
    "    # `None` values will be filtered out\n",
    "    f = [o for o,m in zip(items,mask) if m==False]\n",
    "    t = [o for o,m in zip(items,mask) if m==True ]\n",
    "    return f,t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SplitData():\n",
    "    def __init__(self, train, valid): self.train,self.valid = train,valid\n",
    "        \n",
    "    def __getattr__(self,k): return getattr(self.train,k)\n",
    "    #This is needed if we want to pickle SplitData and be able to load it back without recursion errors\n",
    "    def __setstate__(self,data:Any): self.__dict__.update(data) \n",
    "    \n",
    "    @classmethod\n",
    "    def split_by_func(cls, il, f):\n",
    "        lists = map(il.new, split_by_func(il.items, f))\n",
    "        return cls(*lists)\n",
    "\n",
    "    def __repr__(self): return f'{self.__class__.__name__}\\nTrain: {self.train}\\nValid: {self.valid}\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from collections import OrderedDict\n",
    "\n",
    "def uniqueify(x, sort=False):\n",
    "    res = list(OrderedDict.fromkeys(x).keys())\n",
    "    if sort: res.sort()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Processor(): \n",
    "    def process(self, items): return items\n",
    "\n",
    "class CategoryProcessor(Processor):\n",
    "    def __init__(self): self.vocab=None\n",
    "    \n",
    "    def __call__(self, items):\n",
    "        #The vocab is defined on the first use.\n",
    "        if self.vocab is None:\n",
    "            self.vocab = uniqueify(items)\n",
    "            self.otoi  = {v:k for k,v in enumerate(self.vocab)}\n",
    "        return [self.proc1(o) for o in items]\n",
    "    def proc1(self, item):  return self.otoi[item]\n",
    "    \n",
    "    def deprocess(self, idxs):\n",
    "        assert self.vocab is not None\n",
    "        return [self.deproc1(idx) for idx in idxs]\n",
    "    def deproc1(self, idx): return self.vocab[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parent_labeler(fn): return fn.parent.name\n",
    "\n",
    "def _label_by_func(ds, f, cls=ItemList): return cls([f(o) for o in ds.items], path=ds.path)\n",
    "\n",
    "#This is a slightly different from what was seen during the lesson,\n",
    "#   we'll discuss the changes in lesson 11\n",
    "class LabeledData():\n",
    "    def process(self, il, proc): return il.new(compose(il.items, proc))\n",
    "\n",
    "    def __init__(self, x, y, proc_x=None, proc_y=None):\n",
    "        self.x,self.y = self.process(x, proc_x),self.process(y, proc_y)\n",
    "        self.proc_x,self.proc_y = proc_x,proc_y\n",
    "        \n",
    "    def __repr__(self): return f'{self.__class__.__name__}\\nx: {self.x}\\ny: {self.y}\\n'\n",
    "    def __getitem__(self,idx): return self.x[idx],self.y[idx]\n",
    "    def __len__(self): return len(self.x)\n",
    "    \n",
    "    def x_obj(self, idx): return self.obj(self.x, idx, self.proc_x)\n",
    "    def y_obj(self, idx): return self.obj(self.y, idx, self.proc_y)\n",
    "    \n",
    "    def obj(self, items, idx, procs):\n",
    "        isint = isinstance(idx, int) or (isinstance(idx,torch.LongTensor) and not idx.ndim)\n",
    "        item = items[idx]\n",
    "        for proc in reversed(listify(procs)):\n",
    "            item = proc.deproc1(item) if isint else proc.deprocess(item)\n",
    "        return item\n",
    "\n",
    "    @classmethod\n",
    "    def label_by_func(cls, il, f, proc_x=None, proc_y=None):\n",
    "        return cls(il, _label_by_func(il, f), proc_x=proc_x, proc_y=proc_y)\n",
    "\n",
    "def label_by_func(sd, f, proc_x=None, proc_y=None):\n",
    "    train = LabeledData.label_by_func(sd.train, f, proc_x=proc_x, proc_y=proc_y)\n",
    "    valid = LabeledData.label_by_func(sd.valid, f, proc_x=proc_x, proc_y=proc_y)\n",
    "    return SplitData(train,valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResizeFixed(Transform):\n",
    "    _order=10\n",
    "    def __init__(self,size):\n",
    "        if isinstance(size,int): size=(size,size)\n",
    "        self.size = size\n",
    "        \n",
    "    def __call__(self, item): return item.resize(self.size, PIL.Image.BILINEAR)\n",
    "\n",
    "def to_byte_tensor(item):\n",
    "    res = torch.ByteTensor(torch.ByteStorage.from_buffer(item.tobytes()))\n",
    "    w,h = item.size\n",
    "    return res.view(h,w,-1).permute(2,0,1)\n",
    "to_byte_tensor._order=20\n",
    "\n",
    "def to_float_tensor(item): return item.float().div_(255.)\n",
    "to_float_tensor._order=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def show_image(im, figsize=(3,3)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(im.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DataBunch():\n",
    "    def __init__(self, train_dl, valid_dl, c_in=None, c_out=None):\n",
    "        self.train_dl,self.valid_dl,self.c_in,self.c_out = train_dl,valid_dl,c_in,c_out\n",
    "\n",
    "    @property\n",
    "    def train_ds(self): return self.train_dl.dataset\n",
    "\n",
    "    @property\n",
    "    def valid_ds(self): return self.valid_dl.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def databunchify(sd, bs, c_in=None, c_out=None, **kwargs):\n",
    "    dls = get_dls(sd.train, sd.valid, bs, **kwargs)\n",
    "    return DataBunch(*dls, c_in=c_in, c_out=c_out)\n",
    "\n",
    "SplitData.to_databunch = databunchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def normalize_chan(x, mean, std):\n",
    "    return (x-mean[...,None,None]) / std[...,None,None]\n",
    "\n",
    "_m = tensor([0.47, 0.48, 0.45])\n",
    "_s = tensor([0.29, 0.28, 0.30])\n",
    "norm_imagenette = partial(normalize_chan, mean=_m.cuda(), std=_s.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import math\n",
    "def prev_pow_2(x): return 2**math.floor(math.log2(x))\n",
    "\n",
    "def get_cnn_layers(data, nfs, layer, **kwargs):\n",
    "    def f(ni, nf, stride=2): return layer(ni, nf, 3, stride=stride, **kwargs)\n",
    "    l1 = data.c_in\n",
    "    l2 = prev_pow_2(l1*3*3)\n",
    "    layers =  [f(l1  , l2  , stride=1),\n",
    "               f(l2  , l2*2, stride=2),\n",
    "               f(l2*2, l2*4, stride=2)]\n",
    "    nfs = [l2*4] + nfs\n",
    "    layers += [f(nfs[i], nfs[i+1]) for i in range(len(nfs)-1)]\n",
    "    layers += [nn.AdaptiveAvgPool2d(1), Lambda(flatten), \n",
    "               nn.Linear(nfs[-1], data.c_out)]\n",
    "    return layers\n",
    "\n",
    "def get_cnn_model(data, nfs, layer, **kwargs):\n",
    "    return nn.Sequential(*get_cnn_layers(data, nfs, layer, **kwargs))\n",
    "\n",
    "def get_learn_run(nfs, data, lr, layer, cbs=None, opt_func=None, **kwargs):\n",
    "    model = get_cnn_model(data, nfs, layer, **kwargs)\n",
    "    init_cnn(model)\n",
    "    return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def model_summary(run, learn, data, find_all=False):\n",
    "    xb,yb = get_batch(data.valid_dl, run)\n",
    "    device = next(learn.model.parameters()).device#Model may not be on the GPU yet\n",
    "    xb,yb = xb.to(device),yb.to(device)\n",
    "    mods = find_modules(learn.model, is_lin_layer) if find_all else learn.model.children()\n",
    "    f = lambda hook,mod,inp,out: print(f\"{mod}\\n{out.shape}\\n\")\n",
    "    with Hooks(mods, f) as hooks: learn.model(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 08_data_block.ipynb to exp/nb_08.py\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py 08_data_block.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
