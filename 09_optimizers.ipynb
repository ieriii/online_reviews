{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_08 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sgd_step(p, lr, **kwargs):\n",
    "    p.data.add_(-lr, p.grad.data)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Recorder(Callback):\n",
    "    def begin_fit(self): self.lrs,self.losses = [],[]\n",
    "\n",
    "    def after_batch(self):\n",
    "        if not self.in_train: return\n",
    "        self.lrs.append(self.opt.hypers[-1]['lr'])\n",
    "        self.losses.append(self.loss.detach().cpu())        \n",
    "\n",
    "    def plot_lr  (self): plt.plot(self.lrs)\n",
    "    def plot_loss(self): plt.plot(self.losses)\n",
    "        \n",
    "    def plot(self, skip_last=0):\n",
    "        losses = [o.item() for o in self.losses]\n",
    "        n = len(losses)-skip_last\n",
    "        plt.xscale('log')\n",
    "        plt.plot(self.lrs[:n], losses[:n])\n",
    "\n",
    "class ParamScheduler(Callback):\n",
    "    _order=1\n",
    "    def __init__(self, pname, sched_funcs):\n",
    "        self.pname,self.sched_funcs = pname,listify(sched_funcs)\n",
    "\n",
    "    def begin_batch(self): \n",
    "        if not self.in_train: return\n",
    "        fs = self.sched_funcs\n",
    "        if len(fs)==1: fs = fs*len(self.opt.param_groups)\n",
    "        pos = self.n_epochs/self.epochs\n",
    "        for f,h in zip(fs,self.opt.hypers): h[self.pname] = f(pos)\n",
    "            \n",
    "class LR_Find(Callback):\n",
    "    _order=1\n",
    "    def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10):\n",
    "        self.max_iter,self.min_lr,self.max_lr = max_iter,min_lr,max_lr\n",
    "        self.best_loss = 1e9\n",
    "        \n",
    "    def begin_batch(self): \n",
    "        if not self.in_train: return\n",
    "        pos = self.n_iter/self.max_iter\n",
    "        lr = self.min_lr * (self.max_lr/self.min_lr) ** pos\n",
    "        for pg in self.opt.hypers: pg['lr'] = lr\n",
    "            \n",
    "    def after_step(self):\n",
    "        if self.n_iter>=self.max_iter or self.loss>self.best_loss*10:\n",
    "            raise CancelTrainException()\n",
    "        if self.loss < self.best_loss: self.best_loss = self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def weight_decay(p, lr, wd, **kwargs):\n",
    "    p.data.mul_(1 - lr*wd)\n",
    "    return p\n",
    "weight_decay._defaults = dict(wd=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def l2_reg(p, lr, wd, **kwargs):\n",
    "    p.grad.data.add_(wd, p.data)\n",
    "    return p\n",
    "l2_reg._defaults = dict(wd=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def maybe_update(os, dest, f):\n",
    "    for o in os:\n",
    "        for k,v in f(o).items():\n",
    "            if k not in dest: dest[k] = v\n",
    "\n",
    "def get_defaults(d): return getattr(d,'_defaults',{})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Optimizer():\n",
    "    def __init__(self, params, steppers, **defaults):\n",
    "        self.steppers = listify(steppers)\n",
    "        maybe_update(self.steppers, defaults, get_defaults)\n",
    "        # might be a generator\n",
    "        self.param_groups = list(params)\n",
    "        # ensure params is a list of lists\n",
    "        if not isinstance(self.param_groups[0], list): self.param_groups = [self.param_groups]\n",
    "        self.hypers = [{**defaults} for p in self.param_groups]\n",
    "\n",
    "    def grad_params(self):\n",
    "        return [(p,hyper) for pg,hyper in zip(self.param_groups,self.hypers)\n",
    "            for p in pg if p.grad is not None]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p,hyper in self.grad_params():\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "\n",
    "    def step(self):\n",
    "        for p,hyper in self.grad_params(): compose(p, self.steppers, **hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "sgd_opt = partial(Optimizer, steppers=[weight_decay, sgd_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class StatefulOptimizer(Optimizer):\n",
    "    def __init__(self, params, steppers, stats=None, **defaults): \n",
    "        self.stats = listify(stats)\n",
    "        maybe_update(self.stats, defaults, get_defaults)\n",
    "        super().__init__(params, steppers, **defaults)\n",
    "        self.state = {}\n",
    "        \n",
    "    def step(self):\n",
    "        for p,hyper in self.grad_params():\n",
    "            if p not in self.state:\n",
    "                #Create a state for p and call all the statistics to initialize it.\n",
    "                self.state[p] = {}\n",
    "                maybe_update(self.stats, self.state[p], lambda o: o.init_state(p))\n",
    "            state = self.state[p]\n",
    "            for stat in self.stats: state = stat.update(p, state, **hyper)\n",
    "            compose(p, self.steppers, **state, **hyper)\n",
    "            self.state[p] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Stat():\n",
    "    _defaults = {}\n",
    "    def init_state(self, p): raise NotImplementedError\n",
    "    def update(self, p, state, **kwargs): raise NotImplementedError    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def momentum_step(p, lr, grad_avg, **kwargs):\n",
    "    p.data.add_(-lr, grad_avg)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def lin_comb(v1, v2, beta): return beta*v1 + (1-beta)*v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AverageGrad(Stat):\n",
    "    _defaults = dict(mom=0.9)\n",
    "    \n",
    "    def __init__(self, dampening:bool=False): self.dampening=dampening\n",
    "    def init_state(self, p): return {'grad_avg': torch.zeros_like(p.grad.data)}\n",
    "    def update(self, p, state, mom, **kwargs):\n",
    "        state['mom_damp'] = 1-mom if self.dampening else 1.\n",
    "        state['grad_avg'].mul_(mom).add_(state['mom_damp'], p.grad.data)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AverageSqrGrad(Stat):\n",
    "    _defaults = dict(sqr_mom=0.99)\n",
    "    \n",
    "    def __init__(self, dampening:bool=True): self.dampening=dampening\n",
    "    def init_state(self, p): return {'sqr_avg': torch.zeros_like(p.grad.data)}\n",
    "    def update(self, p, state, sqr_mom, **kwargs):\n",
    "        state['sqr_damp'] = 1-sqr_mom if self.dampening else 1.\n",
    "        state['sqr_avg'].mul_(sqr_mom).addcmul_(state['sqr_damp'], p.grad.data, p.grad.data)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class StepCount(Stat):\n",
    "    def init_state(self, p): return {'step': 0}\n",
    "    def update(self, p, state, **kwargs):\n",
    "        state['step'] += 1\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def debias(mom, damp, step): return damp * (1 - mom**step) / (1-mom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def adam_step(p, lr, mom, mom_damp, step, sqr_mom, sqr_damp, grad_avg, sqr_avg, eps, **kwargs):\n",
    "    debias1 = debias(mom,     mom_damp, step)\n",
    "    debias2 = debias(sqr_mom, sqr_damp, step)\n",
    "    p.data.addcdiv_(-lr / debias1, grad_avg, (sqr_avg/debias2).sqrt() + eps)\n",
    "    return p\n",
    "adam_step._defaults = dict(eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def adam_opt(xtra_step=None, **kwargs):\n",
    "    return partial(StatefulOptimizer, steppers=[adam_step,weight_decay]+listify(xtra_step),\n",
    "                   stats=[AverageGrad(dampening=True), AverageSqrGrad(), StepCount()], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 09_optimizers.ipynb to exp/nb_09.py\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py 09_optimizers.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
